{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hola\n"
     ]
    }
   ],
   "source": [
    "print (\"hola\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se importa los modulos necesarios a utilizar\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import datetime as dt\n",
    "from time import strftime\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# En base a la escasa información que se tiene sobre el proceso de extracción de los datos\n",
    "# y dado que se presentan en multiples formatos e inconsistencias, supuse que a la hora de\n",
    "# continuar con la extracción de futuros datos se realizaría por los mismos medios, generando\n",
    "# archivos con formatos e inconsistencias similares, por lo que lo mas adecuado teniendo en cuenta\n",
    "# el tiempo disponible fue realizar una unica función capaz de abrir todos los archivos contenidos\n",
    "# en un directorio, sortearlos según su formato y aplicarle las transformaciones necesarias para\n",
    "# normalizar los datos y el formato; y ademas intentar corregir la mayor cantidad de inconsistencias.\n",
    "\n",
    "\n",
    "# En el futuro y con algo mas de tiempo es posible separar la función en multiples mas especificas para abarcar mas variaciones\n",
    "def file_format(filepath):\n",
    "    \n",
    "    # Separa el archivo en nombre y formato \n",
    "    file = os.path.basename(filepath)\n",
    "    split_tup = os.path.splitext(file)\n",
    "    file_name = split_tup[0]\n",
    "    file_extension = split_tup[1]\n",
    "    \n",
    "    #XLSX (multiples hojas) a CSV (CSV por hoja)\n",
    "    if file_extension == \".xlsx\":\n",
    "        \n",
    "        # For loop por cada Hoja del XLSX\n",
    "        for sheet_name, df in pd.read_excel(filepath, sheet_name=None, dtype= {\"precio\":str}).items():  \n",
    "            \n",
    "            #Normalización del orden de columnas\n",
    "            col = df.pop(\"producto_id\")\n",
    "            df.insert(1, \"producto_id\", col)  \n",
    "            \n",
    "            # Transformar la columna \"sucursal_id\", ya que algunos datos se encuentran como Datetime y están rotados\n",
    "            # Se usa Try/Except ya que no todas las hojas presentan estas inconsistencias.\n",
    "            try:\n",
    "                cont = -1\n",
    "                df[\"nuevo\"] = \"\"\n",
    "                for a in df[\"sucursal_id\"]:  #Inicialmente se intentó usar una unica condición en la serie, solo se logró con un for loop.\n",
    "                    cont+=1\n",
    "                    if type(a) == dt.datetime:\n",
    "                        df[\"nuevo\"].iloc[cont] = df[\"sucursal_id\"].iloc[cont].strftime(\"%d-%m-%Y\")  # strftime rota el dato y transforma a string\n",
    "                df2 = df[\"nuevo\"].str.split(\"-\", expand=True)                                       # Se guarda en una columna \"nueva\" para no interferir\n",
    "                df2 =df2.replace(\"\",None)                                                           # con el for loop\n",
    "                df2 = df2.fillna(value=0).astype(int) # Transformar a INT para eliminar Zeros iniciales        \n",
    "                df[\"sucursal_id\"][df2[2] != 0] = df2[[0,1,2]].astype(str).apply(lambda x: '-'.join(x), axis=1)\n",
    "            except:\n",
    "                pass\n",
    "            del df[\"nuevo\"]\n",
    "            \n",
    "            # Se eliminan guiones y decimales no deseados en columna \"producto_id\"\n",
    "            # Completar las 13 cifras del codigo EAN con zeros iniciales \n",
    "            df.loc[df['producto_id'].notnull(), 'producto_id'] = df.loc[df['producto_id'].notnull(), 'producto_id'].astype(str).str.split(\"-\").str[-1]\n",
    "            df[\"producto_id\"] = df[\"producto_id\"].str.split(\".\").str[0]  \n",
    "            df[\"producto_id\"] = df[\"producto_id\"].str.zfill(13)\n",
    "            \n",
    "            # Eliminar duplicados\n",
    "            df = df.drop_duplicates()\n",
    "            \n",
    "            # Guardar el archivo en formato .CSV\n",
    "            df.to_csv(f\"{sheet_name}.csv\", encoding=\"utf-8\", index=False)\n",
    "        \n",
    "    #JSON a CSV\n",
    "    elif file_extension == \".json\":   \n",
    "        df = pd.read_json(filepath)\n",
    "        df[\"producto_id\"] = df[\"producto_id\"].astype(str)\n",
    "        df[\"producto_id\"] = df['producto_id'].str.split('-').str[-1] # Se eliminan guiones y decimales no deseados en columna \"producto_id\"\n",
    "        df[\"producto_id\"] = df[\"producto_id\"].str.zfill(13)          # Completar las 13 cifras del codigo EAN con zeros iniciales \n",
    "        df = df.drop_duplicates()                                    # Eliminar duplicados\n",
    "        df.to_csv(file_name + \".csv\", encoding =\"utf-8\", index=False)# Guardar el archivo en formato .CSV\n",
    "\n",
    "    #TXT a CSV. Se realiza la misma transformación realizada para el JSON\n",
    "    elif file_extension == \".txt\":\n",
    "        df = pd.read_csv(filepath, delimiter=\"|\")\n",
    "        df[\"producto_id\"] = df['producto_id'].str.split('-').str[-1]\n",
    "        df[\"producto_id\"] = df[\"producto_id\"].str.zfill(13)\n",
    "        df = df.drop_duplicates()\n",
    "        df.to_csv(file_name + \".csv\", encoding=\"utf-8\", index=False)\n",
    "    \n",
    "    #PARQUET a CSV. Se realiza la misma transformación realizada para el JSON   \n",
    "    elif file_extension == \".parquet\":\n",
    "        df = pd.read_parquet(filepath)\n",
    "        df[\"id\"] = df['id'].str.split('-').str[-1]\n",
    "        df[\"id\"] = df[\"id\"].str.zfill(13)\n",
    "        df = df.drop_duplicates(subset=\"id\")\n",
    "        df.to_csv(file_name + \".csv\", encoding= \"utf-8\", index=False)\n",
    "    \n",
    "    # CSV a CSV. Ya que vienen con encodings diferentes se realiza la lectura 2 veces \n",
    "    # Se realiza la misma transformación realizada para el JSON     \n",
    "    elif file_extension == \".csv\":\n",
    "        encodings = [\"utf-8\", \"utf-16\"]\n",
    "        for encode in encodings:\n",
    "            try:\n",
    "                df = pd.read_csv(filepath, encoding=encode)\n",
    "                try:\n",
    "                    df[\"producto_id\"] = df['producto_id'].str.split('-').str[-1]\n",
    "                    df[\"producto_id\"] = df[\"producto_id\"].str.zfill(13)\n",
    "                except:\n",
    "                    pass  \n",
    "                df = df.drop_duplicates()             \n",
    "                df.to_csv(file_name + \".csv\", encoding=\"utf8\", index=False)\n",
    "            except:\n",
    "                pass      \n",
    "        \n",
    "    #formato no reconocido, se evitan errores por formatos no añadidos    \n",
    "    else:\n",
    "        print (file_name+file_extension, \"es nuevo formato aun no añadido a la función\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.md es nuevo formato aun no añadido a la función\n"
     ]
    }
   ],
   "source": [
    "#Codigo que abre todos los archivos del directorio y ejecuta en cada uno la función previa\n",
    "path = (\"D:\\Henry\\Repos\\Labs PI\\PI01_DATA_ENGINEERING-main\\Datasets\")\n",
    "for filename in glob.glob(os.path.join(path, '*.*')):\n",
    "   with open(os.path.join(os.getcwd(), filename), 'r') as f:\n",
    "       file_format (filename)\n",
    "\n",
    "\n",
    "# Al final del proceso se obtienen archivos .CSV en el directorio donde se encuentra el jupiter con el codigo\n",
    "# Todos los archivos se encuentran normalizados y con gran mayoria de correcciones listos para ser cargados\n",
    "# a la base de datos deseada"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2469a70536e4d2335a2ea8907942d0699c37342a371ac185bdb5b0aa6f073890"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
